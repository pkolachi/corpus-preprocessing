#!/usr/bin/env python3
"""
marlin_count, extract bigram staitics from a file.
"""

import optparse
import collections
import gzip
import bz2
import sys;
import itertools;
from itertools import chain;
import time;

CHUNK_SIZE = 1000000000;


def myopen(filename, mode='r'):
    """
    Open file. Use gzip or bzip2 if appropriate.
    """
    if filename.endswith('.gz'):
        return gzip.open(filename, mode)

    if filename.endswith('.bz2'):
        return bz2.BZ2File(filename, mode)

    return open(filename, mode)


def load_counts_tmpfile(file_handle):
    bigram_counts = collections.defaultdict(lambda : collections.defaultdict(int));
    for number, line in enumerate(file_handle):
        columns = line.split();
        entries = ((int(e[0]), int(e[1])) for e in columns);
        for column, value in entries:
            bigram_counts[number][column] = value;
    return bigram_counts;

def read_unigram_counts(file_handle, sent_limit=-1):
    """
    Read unigram counts from text file.
    If sentence_limit is positive only read that-many sentences/lines.
    """
    unigram_counts = collections.defaultdict(int)
    for number, line in enumerate(file_handle):
        tokens = line.split()
        for token in tokens:
            unigram_counts[token] += 1
        if sent_limit >= 0 and sent_limit > number:
            break
    return unigram_counts


def read_unigram_counts_lazy(file_handle, sent_limit=-1):
    """
    Read unigram counts from text file.
    If sentence_limit is positive only read that-many sentences/lines.
    """
    tokens = (token for number, line in enumerate(file_handle) for token in line.split());
    unigram_counts = collections.defaultdict(int);
    for tok in tokens:
        unigram_counts[tok] += 1;
    return unigram_counts;


def write_words(file_handle, table):
    """
    Write words in order of their index to file.
    """
    table_inv = [None] * len(table)
    for word, index in table.items():
        table_inv[index] = word
    for index, word in enumerate(table_inv):
        file_handle.write(word)
        file_handle.write('\n')


def write_words_new(file_handle, table):
    """
    Write words in order of their index to file.
    """
    table_inv = dict((table[word], word) for word in table);
    for index in range(len(table_inv)):
        print >>file_handle, table_inv[index]


def read_bigram_counts(file_handle, table, stop_index, rare_index, sent_limit):
    """
    Count bigram coocurrences. Use rare_index for words not in table.
    If sentence_limit is positive only read that-many sentences/lines.
    """
    counts = []
    for _ in table:
        counts.append(collections.defaultdict(int))
    for number, line in enumerate(file_handle):
        tokens = line.split()
        last = stop_index
        for token in tokens:
            current = table.get(token, None)
            if current is None:
                current = rare_index
            counts[last][current] += 1
            last = current
        counts[last][stop_index] += 1
        if sent_limit >= 0 and sent_limit > number:
            break
    return counts


def read_bigram_counts_lazy(file_handle, table, stop_index, rare_index, sent_limit, tmp_fileprefix):
    """
    Count bigram coocurrences. Use rare_index for words not in table.
    If sentence_limit is positive only read that-many sentences/lines.
    """
    #import scipy.sparse as sparse ;
    sequences  = (["<STOP>"] + line.split() + ["<STOP>"] \
                      for number, line in enumerate(file_handle));
    bigrams    = ((seq[i-1], seq[i]) for seq in sequences for i in range(1, len(seq)));
    bigram_idx = ((table.get(x, rare_index), table.get(y, rare_index)) for x, y in bigrams);
    idx = 0 ;
    while True:
        counts = collections.defaultdict(lambda : collections.defaultdict(int));
        buf    = itertools.islice(bigram_idx, CHUNK_SIZE);
        for i, (x,y) in enumerate(buf, start=1):
            counts[x][y] += 1;
        if i < CHUNK_SIZE and idx == 0:
            return counts;
        elif i < CHUNK_SIZE:
	    break;
	else:
            with open('%s.%d' %(tmp_fileprefix, idx), 'w') as tmp_outhandle:
                write_bigram_counts_new(tmp_outhandle, counts);
        idx += 1;

    if idx:
        final_counts = counts; #collections.defaultdict(lambda : collections.defaultdict(int));
        for _ in range(idx+1):
            tmp_counts = readDotMatrix('%s.%d' %(tmp_fileprefix, _));
            for row in tmp_counts:
                for col in tmp_row:
                    final_counts[row][col] += tmp_counts[row][col];
            del tmp_counts;

    return final_counts;


def read_bigram_counts_scipy(file_handle, table, stop_index, rare_index, sent_limit):
    """
    Count bigram coocurrences. Use rare_index for words not in table.
    If sentence_limit is positive only read that-many sentences/lines.
    """
    import scipy.sparse as sparse;
    counts = sparse.dok_matrix((len(table), len(table)+2), dtype=int);
    for number, line in enumerate(file_handle):
        tokens = line.split()
        last = stop_index
        for token in tokens:
            current = table.get(token, None)
            if current is None:
                current = rare_index
            counts[last, current] += 1;
            last = current
        counts[last, stop_index] += 1
        if sent_limit >= 0 and sent_limit > number:
            break
        if not (number+1)%10000000:
            print >>sys.stderr, number,
    print >>sys.stderr, "";
    return counts

		
def write_bigram_counts(file_handle, counts):
    """
    Write bigram statistics to file.
    """
    for _, neighbors in enumerate(counts):
        items = []
        for neighbor, count in neighbors.items():
            items.append('%d:%d' % (neighbor, count))
        print >>file_handle, ' '.join(items)


def write_bigram_counts_new(file_handle, counts):
    """
    Write bigram statistics to file.
    """
    for _ in range(len(counts)+1):
        items = ('%d:%d' %(idx, counts[_][idx]) for idx in counts[_]);
        print >>file_handle, ' '.join(items);


def write_bigram_counts_scipy(file_handle, counts):
    """
    Write bigram statistics to file.
    """
    rowcount, columncount = counts.shape;
    for _ in range(rowcount):
        cols = counts.indices[counts.indptr[_]:counts.indptr[_+1]];
        vals = counts.data[counts.indptr[_]:counts.indptr[_+1]]; 
        items = []
        for neighbor, count in zip(cols, vals):
            items.append('%d:%d' % (neighbor, count))
        print >>file_handle, ' '.join(items)


def main():
    """
    Main function.
    """

    parser = optparse.OptionParser()
    parser.add_option("-t", "--text", dest="text",
                      help="Input text. (one sentence per line, whitespace separated)",
                      metavar="FILE")
    parser.add_option("-w", "--words", dest="words",
                      help="Output: Word list.", metavar="FILE")
    parser.add_option("-b", "--bigrams", dest="bigrams",
                      help="Output: Bigrams counts.", metavar="FILE")
    parser.add_option("-r", "--rank-limit", dest="rank_limit", default=250000,
                      help="If positive, only extract the r most frequent words.",
                      type=int)
    parser.add_option("-s", "--sent-limit", dest="sent_limit", default=-1,
                      help="If positive, only process the s first sentences/lines.",
                      type=int)
    parser.add_option("-n", "--min-count", dest="min_count", default=0,
                      help="If positive, only extract words that occur at least n times.",
                      type=int)

    options, _ = parser.parse_args()

    with myopen(options.text) as file_handle:
        ts1 = time.time();
        unigram_counts = read_unigram_counts(file_handle, options.sent_limit)
        te1 = time.time();
    with myopen(options.text) as file_handle:
        ts2 = time.time();
        unigram_counts2 = read_unigram_counts_lazy(file_handle, options.sent_limit)
        te2 = time.time();
    print te1-ts1, te2-ts2, len(unigram_counts2)-len(unigram_counts), (unigram_counts == unigram_counts2) ;

    table = {};
    stop_index = 0
    rare_index = 1
    table['<STOP>'] = stop_index
    table['<RARE>'] = rare_index

    unigram_counts = unigram_counts2.items()
    unigram_counts.sort(key=lambda (word, count): count, reverse=True)
    # Add high rank words to table
    for word, _ in unigram_counts:
	if options.min_count > 0 and _ < options.min_count:
	    break;
        table[word] = len(table)
        if options.rank_limit >= 0 and len(table) >= options.rank_limit:
            break
    # Don't need this anymore:
    del unigram_counts

    with myopen('%s.orig' %options.words, 'w') as file_handle:
        ts1 = time.time();
        write_words(file_handle, table)
        te1 = time.time();
    with myopen('%s.new' %options.words, 'w') as file_handle:
        ts2 = time.time();
        write_words_new(file_handle, table)
        te2 = time.time();
    print te1-ts1, te2-ts2;


    with myopen(options.text) as file_handle:
        ts1 = time.time();
        bigram_counts = read_bigram_counts(file_handle, table, stop_index, rare_index,
                                           options.sent_limit)
        te1 = time.time();
    with myopen(options.text) as file_handle:
        ts2 = time.time();
        bigram_counts2 = read_bigram_counts_lazy(file_handle, table, stop_index, rare_index,
                                           options.sent_limit, '%s.new.tmp' %options.bigrams)
        te2 = time.time();
    print te1-ts1, te2-ts2, (bigram_counts == bigram_counts2);
    #assert(len(bigram_counts) == len(table));
    #assert(len(bigram_counts2) == len(table));
    
    #bigram_counts = bigram_counts.tocsr();

    with myopen('%s.orig' %options.bigrams, 'w') as file_handle:
        ts1 = time.time();
        write_bigram_counts(file_handle, bigram_counts)
        te1 = time.time();
    with myopen('%s.new' %options.bigrams, 'w') as file_handle:
        ts2 = time.time();
        write_bigram_counts_new(file_handle, bigram_counts2)
        te2 = time.time();

    print te1-ts1, te2-ts2;


if __name__ == '__main__':
    main()

